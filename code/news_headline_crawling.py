
from selenium import webdriver as wd
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By


from bs4 import BeautifulSoup
import requests
import pandas as pd
from copy import deepcopy
import time
import traceback

# 드라이버 초기화
driver = wd.Chrome(service=Service(ChromeDriverManager().install()))
driver.implicitly_wait(20)
dWait = WebDriverWait(driver, 20)

# 파일 경로
file_path = '시간별_많이_crawling.csv'

sht = list()
cnt = 0

dict_url  = {
    'IT/과학': '105/228', 
    '경제': '101/263', 
    '사회': '102/257', 
    '생활/문화': '103/245', 
    '세계' : '104/322', 
    # 스포츠는 ui가 달라서 별도로 진행
    '정치': '100/269'
}
'''
dict_search = {
    # IT/과학
    0: [('201601', 31), ('201602', 23), ('201603', 11), ('201604', 23), ('201605', 14), ('201606', 21), ('201607', 24), ('201608', 31), ('201609', 22), ('201610', 29), ('201611', 31), ('201612', 33), ('201701', 20), ('201702', 11), ('201703', 7), ('201704', 5), ('201705', 11), ('201706', 21), ('201707', 23), ('201708', 10), ('201709', 17), ('201710', 10), ('201711', 3), ('201712', 20), ('201801', 17), ('201803', 24), ('201804', 20), ('201805', 19), ('201806', 29), ('201807', 20), ('201808', 32), ('201809', 22), ('201810', 26), ('201811', 23), ('201812', 22), ('201901', 26), ('201902', 31), ('201903', 26), ('201904', 18), ('201905', 25), ('201906', 19), ('201907', 27), ('201908', 22), ('201909', 27), ('201910', 20), ('201911', 22), ('201912', 25), ('202001', 38), ('202002', 39), ('202003', 40), ('202004', 38), ('202006', 40), ('202007', 37), ('202008', 40), ('202009', 38), ('202010', 37), ('202011', 38)],
    # 경제
    1 : [('201601', 5), ('201602', 13), ('201603', 21), ('201604', 21), ('201605', 20), ('201606', 19), ('201607', 25), ('201608', 21), ('201609', 18), ('201610', 15), ('201611', 19), ('201612', 27), ('201701', 35), ('201702', 30), ('201703', 30), ('201704', 32), ('201705', 33), ('201706', 33), ('201707', 30), ('201708', 28), ('201709', 32), ('201710', 29), ('201711', 25), ('201712', 35), ('201801', 7), ('201803', 13), ('201804', 15), ('201805', 21), ('201806', 20), ('201807', 6), ('201808', 8), ('201809', 19), ('201810', 19), ('201811', 9), ('201812', 10), ('201901', 19), ('201902', 18), ('201903', 18), ('201904', 23), ('201905', 14), ('201906', 28), ('201907', 14), ('201908', 20), ('201909', 21), ('201910', 24), ('201911', 23), ('201912', 24), ('202001', 30), ('202002', 27), ('202003', 21), ('202004', 32), ('202005', 29), ('202006', 28), ('202007', 32), ('202008', 33), ('202009', 29), ('202010', 26), ('202011', 31), ('202012', 33)],
    # 사회
    2 : [('201601', 22), ('201602', 27), ('201603', 32), ('201604', 27), ('201605', 23), ('201606', 27), ('201607', 27), ('201608', 28), ('201609', 33), ('201610', 30), ('201611', 27), ('201612', 26), ('201701', 29), ('201702', 26), ('201703', 23), ('201704', 29), ('201705', 27), ('201706', 33), ('201707', 29), ('201708', 25), ('201709', 28), ('201710', 30), ('201711', 31), ('201712', 34), ('201801', 10), ('201802', 7), ('201803', 20), ('201804', 16), ('201805', 24), ('201806', 28), ('201807', 17), ('201808', 21), ('201809', 24), ('201810', 15), ('201811', 15), ('201812', 18), ('201901', 17), ('201902', 23), ('201903', 10), ('201904', 9), ('201905', 18), ('201906', 27), ('201907', 19), ('201908', 18), ('201909', 18), ('201910', 13), ('201911', 18), ('201912', 16), ('202001', 25), ('202002', 17), ('202003', 14), ('202004', 15), ('202005', 13), ('202006', 4), ('202007', 5), ('202008', 7), ('202009', 13), ('202010', 5), ('202011', 8)],
    # 생활/문화
    3 : [('201601', 27), ('201602', 32), ('201603', 33), ('201604', 29), ('201605', 28), ('201606', 33), ('201607', 26), ('201608', 23), ('201609', 34), ('201610', 30), ('201611', 36), ('201612', 32), ('201701', 25), ('201702', 14), ('201703', 9), ('201704', 16), ('201705', 22), ('201706', 23), ('201707', 12), ('201708', 21), ('201709', 9), ('201710', 14), ('201711', 8), ('201712', 7), ('201802', 21), ('201803', 18), ('201804', 16), ('201805', 18), ('201806', 20), ('201807', 25), ('201808', 9), ('201809', 23), ('201810', 20), ('201811', 8), ('201812', 25), ('201901', 28), ('201902', 22), ('201903', 23), ('201904', 21), ('201905', 25), ('201906', 22), ('201907', 23), ('201908', 20), ('201909', 25), ('201910', 34), ('201911', 17), ('201912', 27), ('202001', 41), ('202003', 41), ('202005', 37), ('202006', 41), ('202007', 41), ('202008', 41), ('202009', 41), ('202010', 41), ('202011', 41), ('202012', 38)],
    # 세계
    4 : [('201601', 54), ('201602', 59), ('201603', 53), ('201604', 65), ('201605', 52), ('201606', 56), ('201607', 48), ('201608', 59), ('201609', 54), ('201610', 57), ('201611', 64), ('201612', 70), ('201701', 74), ('201702', 74), ('201703', 75), ('201704', 69), ('201705', 72), ('201706', 72), ('201707', 72), ('201708', 74), ('201709', 73), ('201710', 72), ('201711', 74), ('201712', 72), ('201801', 71), ('201802', 73), ('201803', 74), ('201804', 72), ('201805', 74), ('201806', 74), ('201807', 75), ('201808', 74), ('201809', 75), ('201810', 74), ('201811', 75), ('201812', 74), ('201901', 14), ('201902', 29), ('201903', 34), ('201904', 7), ('201905', 22), ('201906', 5), ('201907', 16), ('201908', 14), ('201909', 14), ('201911', 33), ('201912', 17), ('202002', 75), ('202004', 75), ('202005', 75), ('202007', 75), ('202009', 75), ('202012', 75)],
    # 스포츠
    5 : [('201602', 56), ('201603', 55), ('201606', 56), ('201607', 55), ('201609', 56), ('201612', 56), ('201701', 39), ('201702', 46), ('201703', 52), ('201704', 45), ('201705', 38), ('201706', 44), ('201707', 45), ('201708', 41), ('201709', 45), ('201710', 40), ('201711', 49), ('201712', 39), ('201801', 18), ('201802', 32), ('201803', 1), ('201804', 4), ('201805', 13), ('201806', 2), ('201807', 31), ('201808', 23), ('201809', 20), ('201810', 11), ('201812', 15), ('201901', 37), ('201902', 43), ('201903', 24), ('201904', 27), ('201905', 26), ('201906', 25), ('201907', 26), ('201908', 28), ('201909', 21), ('201910', 38), ('201911', 36), ('201912', 36)],
    # 정치
    6 : [('201601', 1), ('201602', 15), ('201603', 15), ('201604', 19), ('201606', 35), ('201607', 33), ('201608', 30), ('201609', 20), ('201610', 31), ('201611', 31), ('201612', 39), ('201701', 61), ('201702', 58), ('201703', 55), ('201704', 59), ('201705', 56), ('201707', 57), ('201708', 58), ('201709', 60), ('201710', 61), ('201711', 60), ('201712', 61), ('201801', 27), ('201802', 37), ('201803', 35), ('201804', 31), ('201805', 44), ('201806', 43), ('201807', 43), ('201808', 42), ('201809', 17), ('201810', 41), ('201811', 42), ('201812', 45), ('201901', 47), ('201902', 51), ('201903', 50), ('201904', 36), ('201905', 46), ('201906', 49), ('201907', 52), ('201908', 46), ('201909', 51), ('201910', 55), ('201911', 46), ('201912', 43), ('202001', 60), ('202002', 60), ('202006', 60), ('202007', 59), ('202008', 58), ('202009', 60), ('202011', 60), ('202012', 59)]
}
'''

dict_search = {
    0 :[('201601', 90), ('201602', 82), ('201603', 70), ('201604', 82), ('201605', 73), ('201606', 80), ('201607', 83), ('201608', 90), ('201609', 81), ('201610', 88), ('201611', 90), ('201612', 92), ('201701', 79), ('201702', 70), ('201703', 66), ('201704', 64), ('201705', 70), ('201706', 80), ('201707', 82), ('201708', 69), ('201709', 76), ('201710', 69), ('201711', 62), ('201712', 79), ('201801', 76), ('201802', 59), ('201803', 83), ('201804', 79), ('201805', 78), ('201806', 88), ('201807', 79), ('201808', 91), ('201809', 81), ('201810', 85), ('201811', 82), ('201812', 81), ('201901', 85), ('201902', 90), ('201903', 85), ('201904', 77), ('201905', 84), ('201906', 78), ('201907', 86), ('201908', 81), ('201909', 86), ('201910', 79), ('201911', 81), ('201912', 84), ('202001', 97), ('202002', 98), ('202003', 99), ('202004', 97), ('202006', 99), ('202007', 96), ('202008', 99), ('202009', 97), ('202010', 96), ('202011', 97)],
    1 :[('201601', 66), ('201602', 74), ('201603', 82), ('201604', 82), ('201605', 81), ('201606', 80), ('201607', 86), ('201608', 82), ('201609', 79), ('201610', 76), ('201611', 80), ('201612', 88), ('201701', 96), ('201702', 91), ('201703', 91), ('201704', 93), ('201705', 94), ('201706', 94), ('201707', 91), ('201708', 89), ('201709', 93), ('201710', 90), ('201711', 86), ('201712', 96), ('201801', 68), ('201802', 61), ('201803', 74), ('201804', 76), ('201805', 82), ('201806', 81), ('201807', 67), ('201808', 69), ('201809', 80), ('201810', 80), ('201811', 70), ('201812', 71), ('201901', 80), ('201902', 79), ('201903', 79), ('201904', 84), ('201905', 75), ('201906', 89), ('201907', 75), ('201908', 81), ('201909', 82), ('201910', 85), ('201911', 84), ('201912', 85), ('202001', 91), ('202002', 88), ('202003', 82), ('202004', 93), ('202005', 90), ('202006', 89), ('202007', 93), ('202008', 94), ('202009', 90), ('202010', 87), ('202011', 92), ('202012', 94)],
    2 : [('201601', 85), ('201602', 90), ('201603', 95), ('201604', 90), ('201605', 86), ('201606', 90), ('201607', 90), ('201608', 91), ('201609', 96), ('201610', 93), ('201611', 90), ('201612', 89), ('201701', 92), ('201702', 89), ('201703', 86), ('201704', 92), ('201705', 90), ('201706', 96), ('201707', 92), ('201708', 88), ('201709', 91), ('201710', 93), ('201711', 94), ('201712', 97), ('201801', 73), ('201802', 70), ('201803', 83), ('201804', 79), ('201805', 87), ('201806', 91), ('201807', 80), ('201808', 84), ('201809', 87), ('201810', 78), ('201811', 78), ('201812', 81), ('201901', 80), ('201902', 86), ('201903', 73), ('201904', 72), ('201905', 81), ('201906', 90), ('201907', 82), ('201908', 81), ('201909', 81), ('201910', 76), ('201911', 81), ('201912', 79), ('202001', 88), ('202002', 80), ('202003', 77), ('202004', 78), ('202005', 76), ('202006', 67), ('202007', 68), ('202008', 70), ('202009', 76), ('202010', 68), ('202011', 71), ('202012', 63)],
    3 : [('201601', 85), ('201602', 90), ('201603', 91), ('201604', 87), ('201605', 86), ('201606', 91), ('201607', 84), ('201608', 81), ('201609', 92), ('201610', 88), ('201611', 94), ('201612', 90), ('201701', 83), ('201702', 72), ('201703', 67), ('201704', 74), ('201705', 80), ('201706', 81), ('201707', 70), ('201708', 79), ('201709', 67), ('201710', 72), ('201711', 66), ('201712', 65), ('201801', 58), ('201802', 79), ('201803', 76), ('201804', 74), ('201805', 76), ('201806', 78), ('201807', 83), ('201808', 67), ('201809', 81), ('201810', 78), ('201811', 66), ('201812', 83), ('201901', 86), ('201902', 80), ('201903', 81), ('201904', 79), ('201905', 83), ('201906', 80), ('201907', 81), ('201908', 78), ('201909', 83), ('201910', 92), ('201911', 75), ('201912', 85), ('202001', 99), ('202003', 99), ('202005', 95), ('202006', 99), ('202007', 99), ('202008', 99), ('202009', 99), ('202010', 99), ('202011', 99), ('202012', 96)],
    4 : [('201601', 78), ('201602', 83), ('201603', 77), ('201604', 89), ('201605', 76), ('201606', 80), ('201607', 72), ('201608', 83), ('201609', 78), ('201610', 81), ('201611', 88), ('201612', 94), ('201701', 98), ('201702', 98), ('201703', 99), ('201704', 93), ('201705', 96), ('201706', 96), ('201707', 96), ('201708', 98), ('201709', 97), ('201710', 96), ('201711', 98), ('201712', 96), ('201801', 95), ('201802', 97), ('201803', 98), ('201804', 96), ('201805', 98), ('201806', 98), ('201807', 99), ('201808', 98), ('201809', 99), ('201810', 98), ('201811', 99), ('201812', 98), ('201901', 38), ('201902', 53), ('201903', 58), ('201904', 31), ('201905', 46), ('201906', 29), ('201907', 40), ('201908', 38), ('201909', 38), ('201910', 24), ('201911', 57), ('201912', 41), ('202002', 99), ('202004', 99), ('202005', 99), ('202007', 99), ('202009', 99), ('202012', 99)],
    5 : [('201602', 99), ('201603', 98), ('201606', 99), ('201607', 98), ('201609', 99), ('201612', 99), ('201701', 82), ('201702', 89), ('201703', 95), ('201704', 88), ('201705', 81), ('201706', 87), ('201707', 88), ('201708', 84), ('201709', 88), ('201710', 83), ('201711', 92), ('201712', 82), ('201801', 61), ('201802', 75), ('201803', 44), ('201804', 47), ('201805', 56), ('201806', 45), ('201807', 74), ('201808', 66), ('201809', 63), ('201810', 54), ('201811', 43), ('201812', 58), ('201901', 80), ('201902', 86), ('201903', 67), ('201904', 70), ('201905', 69), ('201906', 68), ('201907', 69), ('201908', 71), ('201909', 64), ('201910', 81), ('201911', 79), ('201912', 79)],
    6 : [('201601', 39), ('201602', 53), ('201603', 53), ('201604', 57), ('201605', 38), ('201606', 73), ('201607', 71), ('201608', 68), ('201609', 58), ('201610', 69), ('201611', 69), ('201612', 77), ('201701', 99), ('201702', 96), ('201703', 93), ('201704', 97), ('201705', 94), ('201707', 95), ('201708', 96), ('201709', 98), ('201710', 99), ('201711', 98), ('201712', 99), ('201801', 65), ('201802', 75), ('201803', 73), ('201804', 69), ('201805', 82), ('201806', 81), ('201807', 81), ('201808', 80), ('201809', 55), ('201810', 79), ('201811', 80), ('201812', 83), ('201901', 85), ('201902', 89), ('201903', 88), ('201904', 74), ('201905', 84), ('201906', 87), ('201907', 90), ('201908', 84), ('201909', 89), ('201910', 93), ('201911', 84), ('201912', 81), ('202001', 98), ('202002', 98), ('202006', 98), ('202007', 97), ('202008', 96), ('202009', 98), ('202011', 98), ('202012', 97)]
}

def get_date(url, title):
    response = requests.get(url)
    soup = None

    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

    else : 
        print(response.status_code)
        raise Exception(f'{title} is error \n status_code = {response.status_code}')

    tmp = soup.select_one('#ct > div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div:nth-child(1) > span').text.strip()
    return tmp

try:
    
    # 스포츠 외 나머지
    for idx, (category, category_id) in enumerate(dict_url.items()):

        
        # pg = 1
        target_id = idx if category != '정치' else 6

        # while True:
        for date, crawling_cnt in dict_search[target_id] :
            print(date, crawling_cnt)

            each_count = 0
            day = 1
            
            while each_count <= crawling_cnt:
                

                url = f'https://news.naver.com/breakingnews/section/{category_id}?date={date}{day:02d}'  
                day += 1

                response = requests.get(url)
                html = response.text

                soup = BeautifulSoup(html, 'html.parser')
            
                list_news_elems = soup.select('#newsct > div.section_latest > div > div.section_latest_article._CONTENT_LIST._PERSIST_META > div > ul > li')
                    

                for j, elem in enumerate(list_news_elems):

                    if each_count > crawling_cnt:
                        break

                    if j > 4:
                        break
                    
                    row = dict()
                
                    cnt += 1
                    each_count += 1
                    
                    row['ID'] = f'crawling-naver_train_{cnt:05d}'
                    row['text'] = elem.select_one('div > div > div.sa_text > a > strong').text.strip()
                    row['target'] = idx if category != '정치' else 6
                    row['url'] = elem.select_one('div > div > div.sa_text > a')['href'].strip()
                    row['date'] = elem.select_one('div > div > div.sa_text > div.sa_text_info > div.sa_text_info_left > div.sa_text_datetime > b').text.strip()
                    # row['date'] = get_date(row['url'], row['text'])

                    sht.append(deepcopy(row))
                    del row

                del list_news_elems
        


    

    # 임시
    # sht.append({'ID': '임시', 'text': '서호철 류진욱 야수-투수 연봉 최고 인상률 기록 NC 다이노스 연봉협상 마감', 'url': '임시', 'date': '임시'})

    # 스포츠 한 분야 당 350개씩 -> 총 2800개
    # kbaseball, wbaseball, kfootball, wfootball, basketball, volleyball,golf, general
    sports_category = ['kbaseball', 'wbaseball', 'kfootball', 'wfootball', 'basketball', 'volleyball', 'golf', 'general']
    
    for sport in sports_category:
        print(sport)

        for date, crawling_cnt in dict_search[5] : # 스포츠 target id값임 5
            sport_cnt = int(crawling_cnt/8) if crawling_cnt > 8 else 1 # 스포츠 분야별로 골고루 수집하기 위함...
            print(date, sport_cnt)

            each_count = 0
            day = 1

        # each_count = 0
        # search_date = 20240126

            # 날짜 변경
            while each_count <= sport_cnt:
                pg = 1  
                isStop = False

                oUrl = f'https://sports.news.naver.com/{sport}/news/index?isphoto=N&date={date}{day:02d}'
                day += 1
                
                # 페이지 이동
                while True:
                    print(f'스포츠_{sport} : {pg}')
                    if isStop: 
                        break

                    url = f'{oUrl}&page={pg}'
                    driver.get(url)
                    html = driver.page_source
                    soup = BeautifulSoup(html, 'html.parser')
                    list_news_elems = soup.select('#_newsList > ul > li')

                    if not len(list_news_elems):
                        dWait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#_newsList > ul > li')))
                        html = driver.page_source
                        soup = BeautifulSoup(html, 'html.parser')
                        list_news_elems = soup.select('#_newsList > ul > li')


                    pg += 1

                    # 다음 페이지 체크용
                    # 뉴스 목록에서 제일마지막이 이전까지 중 제일 마지막과 동일하면 같은 페이지. 즉 마지막 페이지. 멈춤
                    if list_news_elems[-1].select_one('div > a > span').text.strip() == sht[-1]['text']:
                        break
                    
                    for j, elem in enumerate(list_news_elems):

                        if each_count > crawling_cnt:
                            isStop = True
                            break

                        if j >= 3:
                            isStop = True
                            break
                        
                        row = dict()
                        cnt += 1
                        each_count += 1

                        row['ID'] = f'crawling-naver_train_{cnt:05d}'
                        row['text'] = elem.select_one('div > a > span').text.strip()
                        row['target'] = 5
                        row_url = elem.select_one('div > a')['href'].strip()
                        row['url'] = f"https://sports.news.naver.com{row_url}"
                        row['date'] = elem.select_one('div > div > span.time').text.strip()
                        
                        sht.append(deepcopy(row))
                        if not sht : print(sht[-1])
                        del row

                    del list_news_elems




    df = pd.DataFrame(sht)
    df.to_csv(f'./{file_path}')
except:
    print(traceback.format_exc())
    df = pd.DataFrame(sht)
    df.to_csv(f'./도중종료_{file_path}')
finally:
    driver.close()







# tmp = {'a': [1, 2, 3, 4], 'b': [5, 6, 7, 8]}
# tmp_list = [{'a': 9, 'b': 10}]
# df = pd.DataFrame(tmp)
# df2 = pd.DataFrame(tmp_list)
# print(pd.concat([df, df2]))